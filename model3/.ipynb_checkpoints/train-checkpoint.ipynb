{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![attention_intent_slot-gate模型](img/model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在《Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling》中的模型attention-based rNN model基础上，提出了slot-gate门。\n",
    "    \n",
    "   通过slot-gate来加强intent与slot任务的交互性。见文章《Slot-Gated Modeling for Joint Slot Filling and Intent Prediction》。\n",
    "    \n",
    "   模型步骤：\n",
    "    \n",
    "   1.意图识别是利用encoder中的最后一个time step中的双向隐层，利用attention加权平均，最后接一个fc层进行分类\n",
    "    \n",
    "   2.槽填充是序列标注，双向隐状态加attention权重，最后接一个fc层分类。\n",
    "    \n",
    "     a.上图a中模型结构，利用了slot attention与intent attention。经过gate门后的值与每个时间步中的slot attention进行交互。\n",
    "        \n",
    "     b.上图b中模型结构，只利用了intent attention。经过gate门后的值与每个时间步的隐状态进行交互。\n",
    "        \n",
    "   3.总的loss = 意图识别loss + 槽填充loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attention-based rnn 模型步骤：\n",
    "    \n",
    "   1.底层是bilstm或bigru，输入为用户语句序列，输出为隐状态\n",
    "    \n",
    "   2.槽填充为序列标注任务，将用户语句序列映射到槽标签中。\n",
    "        \n",
    "   ### slot context vector:\n",
    "   \n",
    "   槽注意力向量是隐状态加权和：\n",
    "   \n",
    "   ![slot-1模型](img/slot_1.png)      (1)\n",
    "    \n",
    "   槽注意力权重：\n",
    "   \n",
    "   ![slot-2模型](img/slot_2.png)      (2)\n",
    "   \n",
    "   最后将隐状态和槽上下文向量$c^S_i$用于预测标签序列：\n",
    "   \n",
    "   ![slot-3模型](img/slot_3.png)      (3)\n",
    "   \n",
    "   这里的slot context vector就是对于每个位置$i$，有一个对应的前馈网络权重，经过前馈网络和激活函数得到$e_i^S$，经过softmax得到$\\alpha^S_i$。再由(1)得到slot上下文向量。\n",
    "   \n",
    "   \n",
    "   3.意图识别为分类任务，用bilstm或bigru最后一个时间步隐状态进行预测。\n",
    "    \n",
    "   ### intent context vector:\n",
    "        \n",
    "   注意力向量的计算和槽注意力向量计算一致：\n",
    "   \n",
    "   ![intent-1模型](img/intent_1.png)      (4)\n",
    "   \n",
    "   注意力权重：\n",
    "   \n",
    "   ![intent-2模型](img/intent_2.png)      (5)\n",
    "   \n",
    "   最后一个隐状态和意图上下文向量$c^I$用于预测意图类别：\n",
    "   \n",
    "   ![intent-3模型](img/intent_3.png)      (6)\n",
    "   \n",
    "   ### slot-gate机制\n",
    "   \n",
    "   ![slot-gate模型](img/gate.png)         (7)\n",
    "   \n",
    "   #### 带slot-attention和intent-attention的gate机制\n",
    "   \n",
    "   slot-gate的计算：\n",
    "   \n",
    "   ![slot-gate1模型](img/gate_1.png)      (8)\n",
    "   \n",
    "   这里的$g$可看做是一个加权特征，那么(3)式中的槽序列预测公式可改为：\n",
    "   \n",
    "   ![slot-gate2模型](img/gate_2.png)      (9)\n",
    "   \n",
    "    g越大，表示slot context vector和intent context vector关注的是输入序列的同一部分，也说明槽与意图之间的相关性更强，则context vector对预测结果的贡献更可靠。\n",
    "    \n",
    "   #### 只带intent-attention的gate机制，将(8)和(9)改为如下：\n",
    "   \n",
    "   ![slot-gate3模型](img/gate_3.png)      (10)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchtext import data, datasets\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "atis_data = os.path.join(base_dir, 'atis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "build train and val dataset\n",
    "'''\n",
    "    \n",
    "tokenize = lambda s:s.split()\n",
    "\n",
    "SOURCE = data.Field(sequential=True, tokenize=tokenize,\n",
    "                    lower=True, use_vocab=True,\n",
    "                    init_token='<sos>', eos_token='<eos>',\n",
    "                    pad_token='<pad>', unk_token='<unk>',\n",
    "                    batch_first=True, fix_length=50,\n",
    "                    include_lengths=True) #include_lengths=True为方便之后使用torch的pack_padded_sequence\n",
    "\n",
    "TARGET = data.Field(sequential=True, tokenize=tokenize,\n",
    "                    lower=True, use_vocab=True,\n",
    "                    init_token='<sos>', eos_token='<eos>',\n",
    "                    pad_token='<pad>', unk_token='<unk>',\n",
    "                    batch_first=True, fix_length=50,\n",
    "                    include_lengths=True) #include_lengths=True为方便之后使用torch的pack_padded_sequence\n",
    "LABEL = data.Field(\n",
    "                sequential=False,\n",
    "                use_vocab=True)\n",
    "\n",
    "train, val = data.TabularDataset.splits(\n",
    "                                        path=atis_data,\n",
    "                                        skip_header=True,\n",
    "                                        train='atis.train.csv',\n",
    "                                        validation='atis.test.csv',\n",
    "                                        format='csv',\n",
    "                                        fields=[('index', None), ('intent', LABEL), ('source', SOURCE), ('target', TARGET)])\n",
    "\n",
    "SOURCE.build_vocab(train, val)\n",
    "TARGET.build_vocab(train, val)\n",
    "LABEL.build_vocab(train, val)\n",
    "\n",
    "train_iter, val_iter = data.Iterator.splits(\n",
    "                                            (train, val),\n",
    "                                            batch_sizes=(32, len(val)), # 训练集设置为64,验证集整个集合用于测试\n",
    "                                            shuffle=True,\n",
    "                                            sort_within_batch=True, #为true则一个batch内的数据会按sort_key规则降序排序\n",
    "                                            sort_key=lambda x: len(x.source)) #这里按src的长度降序排序，主要是为后面pack,pad操作)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOURCE.vocab.stoi[SOURCE.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save source words\n",
    "source_words_path = os.path.join(os.getcwd(), 'source_words.pkl')\n",
    "with open(source_words_path, 'wb') as f_source_words:\n",
    "    pickle.dump(SOURCE.vocab, f_source_words)\n",
    "\n",
    "# save target words\n",
    "target_words_path = os.path.join(os.getcwd(), 'target_words.pkl')\n",
    "with open(target_words_path, 'wb') as f_target_words:\n",
    "    pickle.dump(TARGET.vocab, f_target_words)\n",
    "    \n",
    "# save label words\n",
    "label_words_path = os.path.join(os.getcwd(), 'label_words.pkl')\n",
    "with open(label_words_path, 'wb') as f_label_words:\n",
    "    pickle.dump(LABEL.vocab, f_label_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import math\n",
    "from apex import amp\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "      \n",
    "        \n",
    "# 构建slotgate计算方式，利用slot context与intent context\n",
    "class SlotGate(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SlotGate, self).__init__()\n",
    "        self.fc_intent_context = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_v = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, slot_context, intent_context):\n",
    "        '''\n",
    "        注意这里slot_context是slot上下文context，[batch_size, hidden_dim]，或者是时间步的hidden\n",
    "        intent_context:[batch_size, hidden_dim]\n",
    "        '''\n",
    "        # intent_context_linear:[batch_size, hidden_dim]\n",
    "        intent_context_linear = self.fc_intent_context(intent_context)\n",
    "        \n",
    "        # sum_intent_slot_context:[batch_size, hidden_dim]\n",
    "        sum_intent_slot_context = slot_context + intent_context_linear\n",
    "        \n",
    "        # fc_linear:[batch_size, hidden_dim]\n",
    "        fc_linear = self.fc_v(torch.tanh(sum_intent_slot_context))\n",
    "        \n",
    "        # sum_gate_vec:[batch_size]\n",
    "        sum_gate_vec = torch.sum(fc_linear, dim=1)\n",
    "        \n",
    "        return sum_gate_vec\n",
    "    \n",
    "# 这里计算slot context与intent context。就是bigru每个时间步隐藏特征的加权向量，这里不同于原论文，这里使用点乘来计算注意力权重weight\n",
    "class AttnContext(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttnContext, self).__init__()\n",
    "\n",
    "    def forward(self, hidden, source_output_hidden):\n",
    "        # source_output_hidden:[batch_size, seq_len, hidden_size]\n",
    "        # hidden:[batch_size, hidden_size]\n",
    "        hidden = hidden.unsqueeze(1) # [batch_size, 1, hidden_size]\n",
    "        \n",
    "        attn_weight = torch.sum(hidden * source_output_hidden,dim=2) # [batch_size, seq_len]\n",
    "        \n",
    "        attn_weight = F.softmax(attn_weight, dim=1).unsqueeze(1) # [batch_size, 1, seq_len]\n",
    "        \n",
    "        # 类似于注意力向量\n",
    "        attn_vector = attn_weight.bmm(source_output_hidden) # [batch_size, 1, hidden_size]\n",
    "        \n",
    "        return attn_vector.squeeze(1) # [batch_size, hidden_size]\n",
    "\n",
    "\n",
    "#构建模型\n",
    "class BirnnAttentionGate(nn.Module):\n",
    "    def __init__(self, source_input_dim, source_emb_dim, hidden_dim, n_layers, dropout, pad_index, slot_output_size, intent_output_size, seq_len, predict_flag, slot_attention_flag):\n",
    "        super(BirnnAttentionGate, self).__init__()\n",
    "        self.pad_index = pad_index\n",
    "        self.hidden_dim = hidden_dim//2 # 双向lstm\n",
    "        self.n_layers = n_layers\n",
    "        self.slot_output_size = slot_output_size\n",
    "        # 是否预测模式\n",
    "        self.predict_flag = predict_flag\n",
    "        # 原论文中有两种模型结构，一个带slot_attention，一个不带slot_attention\n",
    "        self.slot_attention_flag = slot_attention_flag\n",
    "        \n",
    "        self.source_embedding = nn.Embedding(source_input_dim, source_emb_dim, padding_idx=pad_index)\n",
    "        # 双向gru，隐层维度是hidden_dim\n",
    "        self.source_gru = nn.GRU(source_emb_dim, self.hidden_dim, n_layers, dropout=dropout, bidirectional=True, batch_first=True) #使用双向\n",
    "        \n",
    "        # slot context\n",
    "        self.slot_context = AttnContext(hidden_dim)\n",
    "        \n",
    "        # intent context\n",
    "        self.intent_context = AttnContext(hidden_dim)\n",
    "        \n",
    "        # slotgate类\n",
    "        self.slotGate = SlotGate(hidden_dim)\n",
    "        \n",
    "        # 意图intent预测\n",
    "        self.intent_output = nn.Linear(hidden_dim, intent_output_size)\n",
    "        \n",
    "        # 槽slot预测\n",
    "        self.slot_output = nn.Linear(hidden_dim, slot_output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, source_input, source_len):\n",
    "        '''\n",
    "        source_input:[batch_size, seq_len]\n",
    "        source_len:[batch_size]\n",
    "        '''\n",
    "        if self.predict_flag:\n",
    "            assert len(source_input) == 1, '预测时一次输入一句话'\n",
    "            seq_len = source_len[0]\n",
    "            \n",
    "            # 将输入的source进行编码\n",
    "            # source_embedded:[batch_size, seq_len, source_emb_dim]\n",
    "            source_embedded = self.source_embedding(source_input)\n",
    "            packed = torch.nn.utils.rnn.pack_padded_sequence(source_embedded, source_len, batch_first=True, enforce_sorted=True) #这里enfore_sotred=True要求数据根据词数排序\n",
    "            source_output, hidden = self.source_gru(packed)\n",
    "            # source_output=[batch_size, seq_len, 2 * self.hidden_size]，这里的2*self.hidden_size = hidden_dim\n",
    "            # hidden=[n_layers * 2, batch_size, self.hidden_size]\n",
    "            source_output, _ = torch.nn.utils.rnn.pad_packed_sequence(source_output, batch_first=True, padding_value=self.pad_index, total_length=len(source_input[0])) #这个会返回output以及压缩后的legnths\n",
    "            \n",
    "            batch_size = source_input.shape[0]\n",
    "            seq_len = source_input.shape[1]\n",
    "            # 保存slot的预测概率\n",
    "            slot_outputs = torch.zeros(batch_size, seq_len, self.slot_output_size).to(device)       \n",
    "                \n",
    "            aligns = source_output.transpose(0,1) # 为了拿到每个时间步的输出特征，即每个时间步的隐藏向量\n",
    "            \n",
    "            output_tokens =[]\n",
    "                \n",
    "            # 槽识别\n",
    "            for t in range(seq_len):\n",
    "                '''\n",
    "                此时刻时间步的输出隐向量\n",
    "                '''\n",
    "                aligned = aligns[t]# [batch_size, hidden_size]\n",
    "                    \n",
    "                # 是否需要计算slot attention\n",
    "                if self.slot_attention_flag:\n",
    "                    \n",
    "                    # [batch_size, hidden_size]\n",
    "                    slot_context = self.slot_context(aligned, source_output)\n",
    "                    \n",
    "                    # [batch_size, hidden_size]，意图上下文向量，利用bigru最后一个时间步的隐状态\n",
    "                    intent_context = self.intent_context(source_output[:,-1,:], source_output)\n",
    "                    \n",
    "                    # gate机制，[batch_size]\n",
    "                    slot_gate = self.slotGate(slot_context, intent_context)\n",
    "                    \n",
    "                    # slot_gate:[batch_size, 1]\n",
    "                    slot_gate = slot_gate.unsqueeze(1)\n",
    "                    \n",
    "                    # slot_context_gate:[batch_size, hidden_dim]\n",
    "                    slot_context_gate = slot_gate * slot_context\n",
    "                    \n",
    "                # 否则，利用每个时间步的隐状态与intent context计算slot gate\n",
    "                else:\n",
    "                     # [batch_size, hidden_size]，意图上下文向量，利用bigru最后一个时间步的隐状态\n",
    "                    intent_context = self.intent_context(source_output[:,-1,:], source_output)\n",
    "                    \n",
    "                    # gate机制，[batch_size]\n",
    "                    slot_gate = self.slotGate(source_output[:,t,:], intent_context)\n",
    "                    \n",
    "                     # slot_gate:[batch_size, 1]\n",
    "                    slot_gate = slot_gate.unsqueeze(1)\n",
    "                    \n",
    "                    # slot_context_gate:[batch_size, hidden_dim]\n",
    "                    slot_context_gate = slot_gate * source_output[:,t,:]\n",
    "                \n",
    "                \n",
    "                \n",
    "                # 预测槽slot, [batch_size, slot_output_size]\n",
    "                slot_prediction = self.slot_output(slot_context_gate + source_output[:,t,:])\n",
    "                slot_outputs[:, t, :] = slot_prediction\n",
    "                \n",
    "                \n",
    "            #意图识别\n",
    "            intent_outputs = self.intent_output(intent_context + source_output[:,-1,:])\n",
    "\n",
    "            return slot_outputs, intent_outputs\n",
    "            \n",
    "        # 训练阶段\n",
    "        else:\n",
    "            # 将输入的source进行编码\n",
    "            # source_embedded:[batch_size, seq_len, source_emb_dim]\n",
    "            source_embedded = self.source_embedding(source_input)\n",
    "            packed = torch.nn.utils.rnn.pack_padded_sequence(source_embedded, source_len, batch_first=True, enforce_sorted=True) #这里enfore_sotred=True要求数据根据词数排序\n",
    "            source_output, hidden = self.source_gru(packed)\n",
    "            # source_output=[batch_size, seq_len, 2 * self.hidden_size]，这里的2*self.hidden_size = hidden_dim\n",
    "            # hidden=[n_layers * 2, batch_size, self.hidden_size]\n",
    "            source_output, _ = torch.nn.utils.rnn.pad_packed_sequence(source_output, batch_first=True, padding_value=self.pad_index, total_length=len(source_input[0])) #这个会返回output以及压缩后的legnths\n",
    "            \n",
    "            batch_size = source_input.shape[0]\n",
    "            seq_len = source_input.shape[1]\n",
    "            # 保存slot的预测概率\n",
    "            slot_outputs = torch.zeros(batch_size, seq_len, self.slot_output_size).to(device)       \n",
    "                \n",
    "            aligns = source_output.transpose(0,1) # 为了拿到每个时间步的输出特征，即每个时间步的隐藏向量\n",
    "                \n",
    "            # 槽识别\n",
    "            for t in range(seq_len):\n",
    "                '''\n",
    "                此时刻时间步的输出隐向量\n",
    "                '''\n",
    "                aligned = aligns[t]# [batch_size, hidden_size]\n",
    "                    \n",
    "                # 是否需要计算slot attention\n",
    "                if self.slot_attention_flag:\n",
    "                    \n",
    "                    # [batch_size, hidden_size]\n",
    "                    slot_context = self.slot_context(aligned, source_output)\n",
    "                    \n",
    "                    # [batch_size, hidden_size]，意图上下文向量，利用bigru最后一个时间步的隐状态\n",
    "                    intent_context = self.intent_context(source_output[:,-1,:], source_output)\n",
    "                    \n",
    "                    # gate机制，[batch_size]\n",
    "                    slot_gate = self.slotGate(slot_context, intent_context)\n",
    "                    \n",
    "                    # slot_gate:[batch_size, 1]\n",
    "                    slot_gate = slot_gate.unsqueeze(1)\n",
    "                    \n",
    "                    # slot_context_gate:[batch_size, hidden_dim]\n",
    "                    slot_context_gate = slot_gate * slot_context\n",
    "                    \n",
    "                # 否则，利用每个时间步的隐状态与intent context计算slot gate\n",
    "                else:\n",
    "                     # [batch_size, hidden_size]，意图上下文向量，利用bigru最后一个时间步的隐状态\n",
    "                    intent_context = self.intent_context(source_output[:,-1,:], source_output)\n",
    "                    \n",
    "                    # gate机制，[batch_size]\n",
    "                    slot_gate = self.slotGate(source_output[:,t,:], intent_context)\n",
    "                    \n",
    "                     # slot_gate:[batch_size, 1]\n",
    "                    slot_gate = slot_gate.unsqueeze(1)\n",
    "                    \n",
    "                    # slot_context_gate:[batch_size, hidden_dim]\n",
    "                    slot_context_gate = slot_gate * source_output[:,t,:]\n",
    "                \n",
    "                \n",
    "                \n",
    "                # 预测槽slot, [batch_size, slot_output_size]\n",
    "                slot_prediction = self.slot_output(slot_context_gate + source_output[:,t,:])\n",
    "                slot_outputs[:, t, :] = slot_prediction\n",
    "                \n",
    "                \n",
    "            #意图识别\n",
    "            intent_outputs = self.intent_output(intent_context + source_output[:,-1,:])\n",
    "\n",
    "            return slot_outputs, intent_outputs\n",
    "        \n",
    "\n",
    "# 构建模型，优化函数，损失函数，学习率衰减函数\n",
    "def build_model(source, target, label, source_emb_dim, hidden_dim, n_layers, dropout, lr, gamma, weight_decay, seq_len):\n",
    "    '''\n",
    "    训练seq2seq model\n",
    "    input与output的维度是字典的大小。\n",
    "    encoder与decoder的embedding与dropout可以不同\n",
    "    网络的层数与hiden/cell状态的size必须相同\n",
    "    '''\n",
    "    input_dim = len(source.vocab) # source 词典大小（即词数量）\n",
    "    output_dim = len(target.vocab) # target 词典大小（即实体类型数量）\n",
    "    label_dim = len(label.vocab) # label 词典大小（即意图类别数量）\n",
    "    \n",
    "    model = BirnnAttentionGate(input_dim, source_emb_dim, hidden_dim, n_layers, dropout, source.vocab.stoi[source.pad_token], output_dim, label_dim, seq_len, False, False).to(device)\n",
    "   \n",
    "    model.apply(init_weights)\n",
    "\n",
    "    # 定义优化函数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) #, weight_decay=weight_decay)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=lr) #, momentum=0.9, nesterov=True)\n",
    "    # 定义lr衰减\n",
    "    #scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "    \n",
    "    '''\n",
    "        当网络的评价指标不在提升的时候，可以通过降低网络的学习率来提高网络性能:\n",
    "        optimer指的是网络的优化器\n",
    "        mode (str) ，可选择‘min’或者‘max’，min表示当监控量停止下降的时候，学习率将减小，max表示当监控量停止上升的时候，学习率将减小。默认值为‘min’\n",
    "        factor 学习率每次降低多少，new_lr = old_lr * factor\n",
    "        patience=10，容忍网路的性能不提升的次数，高于这个次数就降低学习率\n",
    "        verbose（bool） - 如果为True，则为每次更新向stdout输出一条消息。 默认值：False\n",
    "        threshold（float） - 测量新最佳值的阈值，仅关注重大变化。 默认值：1e-4\n",
    "        cooldown(int)： 冷却时间“，当调整学习率之后，让学习率调整策略冷静一下，让模型再训练一段时间，再重启监测模式。\n",
    "        min_lr(float or list):学习率下限，可为 float，或者 list，当有多个参数组时，可用 list 进行设置。\n",
    "        eps(float):学习率衰减的最小值，当学习率变化小于 eps 时，则不调整学习率。\n",
    "    '''\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.1, patience=2, verbose=False)\n",
    "    # 这里忽略<pad>的损失。\n",
    "    target_pad_index = target.vocab.stoi[source.pad_token]\n",
    "    # 定义损失函数(实体识别)\n",
    "    loss_slot = nn.CrossEntropyLoss(ignore_index=target_pad_index)\n",
    "    # 定义损失函数(意图识别)\n",
    "    loss_intent = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return model, optimizer, scheduler, loss_slot, loss_intent\n",
    "\n",
    "\n",
    "# 训练\n",
    "def train(model, iterator, optimizer, loss_slot, loss_intent, clip):\n",
    "    '''\n",
    "    开始训练：\n",
    "        1.得到source与target句子\n",
    "        2.上一批batch的计算梯度归0\n",
    "        3.给模型喂source与target，并得到输出output\n",
    "        4.由于损失函数只适用于带有1维target和2维的input，我们需要用view进行flatten(在计算损失时，从output与target中忽略了第一列<sos>)\n",
    "        5.反向传播计算梯度loss.backward()\n",
    "        6.梯度裁剪，防止梯度爆炸\n",
    "        7.更新模型参数\n",
    "        8.损失值求和(返回所有batch的损失的均值)\n",
    "    '''\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, src_lens = batch.source  # src=[batch_size, seq_len]，这里batch.src返回src和src的长度，因为在使用torchtext.Field时设置include_lengths=True\n",
    "        trg, _ = batch.target  # trg=[batch_size, seq_len]\n",
    "        label = batch.intent # [batch_size]\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        #slot_outputs=[batch_size, trg_len, trg_vocab_size], intetn_outputs=[batch_size, intent_size]\n",
    "        slot_outputs, intent_outputs = model(src, src_lens)\n",
    "        \n",
    "        # 以下在计算损失时，忽略了每个tensor的第一个元素及<sos>\n",
    "        output_dim = slot_outputs.shape[-1]\n",
    "        slot_outputs = slot_outputs[:, 1:, :].reshape(-1, output_dim)  # output=[batch_size * (seq_len - 1), output_dim]\n",
    "        trg = trg[:, 1:].reshape(-1)  # trg=[batch_size * (seq_len - 1)]\n",
    "        loss1 = loss_slot(slot_outputs, trg)\n",
    "        loss2 = loss_intent(intent_outputs, label)\n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss += float(loss.item())\n",
    "        # print('epoch_loss:{}'.format(float(loss.item())))\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "'''\n",
    "评估\n",
    "'''\n",
    "def evaluate(model, iterator, loss_slot, loss_intent):\n",
    "    model.eval()  # 评估模型，切断dropout与batchnorm\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():  # 不更新梯度\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src, src_len = batch.source  # src=[batch_size, seq_len]\n",
    "            trg, _ = batch.target  # trg=[batch_size, seq_len]\n",
    "            label = batch.intent\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            label = label.to(device)\n",
    "            # output=[batch_size, seq_len, output_dim]\n",
    "            slot_outputs, intent_outputs = model(src, src_len)\n",
    "\n",
    "            output_dim = slot_outputs.shape[-1]\n",
    "            slot_outputs = slot_outputs[:, 1:, :].reshape(-1, output_dim)  # output=[batch_size * (seq_len - 1), output_dim]\n",
    "            trg = trg[:, 1:].reshape(-1)  # trg=[batch_size * (seq_len - 1)]\n",
    "            loss1 = loss_slot(slot_outputs, trg)\n",
    "            loss2 = loss_intent(intent_outputs, label)\n",
    "            loss = loss1 + loss2\n",
    "            epoch_loss += float(loss.item())\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "def train_model(model, train_iterator, val_iterator, optimizer, scheduler, loss_slot, loss_intent, n_epochs, clip, model_path, writer):\n",
    "    '''\n",
    "    开始训练我们的模型：\n",
    "    1.每一次epoch，都会检查模型是否达到的最佳的validation loss，如果达到了，就更新\n",
    "    最好的validation loss以及保存模型参数\n",
    "    2.打印每个epoch的loss以及困惑度。\n",
    "    '''\n",
    "    best_valid_loss = float('inf')\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = train(model, train_iterator, optimizer, loss_slot, loss_intent, clip)\n",
    "        writer.add_scalar('loss',train_loss,global_step=epoch+1)\n",
    "        \n",
    "        valid_loss = evaluate(model, val_iterator, loss_slot, loss_intent)\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        scheduler.step(valid_loss)\n",
    "        print('epoch:{},time-mins:{},time-secs:{}'.format(epoch + 1, epoch_mins, epoch_secs))\n",
    "        print('train loss:{},train perplexity:{}'.format(train_loss, math.exp(train_loss)))\n",
    "        print('val loss:{}, val perplexity:{}'.format(valid_loss, math.exp(valid_loss)))\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "    #每个epoch所花时间\n",
    "def epoch_time(start_time, end_time):\n",
    "    run_tim = end_time - start_time\n",
    "    run_mins = int(run_tim / 60)\n",
    "    run_secs = int(run_tim-(run_mins * 60))\n",
    "    return run_mins,run_secs\n",
    "\n",
    "#对所有模块和子模块进行权重初始化\n",
    "def init_weights(model):\n",
    "    for name,param in model.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "epoch:1,time-mins:0,time-secs:34\n",
      "train loss:2.5592420307489543,train perplexity:12.926016078768047\n",
      "val loss:4.523448467254639, val perplexity:92.1528367099022\n",
      "epoch:2,time-mins:0,time-secs:33\n",
      "train loss:1.8103211113275626,train perplexity:6.112409881183006\n",
      "val loss:2.5681984424591064, val perplexity:13.0423067969402\n",
      "epoch:3,time-mins:0,time-secs:34\n",
      "train loss:2.504460421892313,train perplexity:12.236954391425018\n",
      "val loss:3.6400842666625977, val perplexity:38.095046732598476\n",
      "epoch:4,time-mins:0,time-secs:34\n",
      "train loss:2.935249496346865,train perplexity:18.82619961731259\n",
      "val loss:2.69341778755188, val perplexity:14.78211179948531\n",
      "epoch:5,time-mins:0,time-secs:34\n",
      "train loss:5.99002950428388,train perplexity:399.42639454299456\n",
      "val loss:2.0366437435150146, val perplexity:7.6648408087339455\n",
      "epoch:6,time-mins:0,time-secs:34\n",
      "train loss:2.136132537554472,train perplexity:8.466629855357192\n",
      "val loss:2.620461940765381, val perplexity:13.742070141497583\n",
      "epoch:7,time-mins:0,time-secs:34\n",
      "train loss:2.3775171710130496,train perplexity:10.778109413441157\n",
      "val loss:2.8913421630859375, val perplexity:18.01747577133412\n",
      "epoch:8,time-mins:0,time-secs:34\n",
      "train loss:2.013664355644813,train perplexity:7.490715764484488\n",
      "val loss:2.949159622192383, val perplexity:19.089904253480842\n",
      "epoch:9,time-mins:0,time-secs:34\n",
      "train loss:1.246872250850384,train perplexity:3.4794430951482562\n",
      "val loss:1.776378870010376, val perplexity:5.908422468660264\n",
      "epoch:10,time-mins:0,time-secs:34\n",
      "train loss:1.1804047952859829,train perplexity:3.25569182489142\n",
      "val loss:1.8017078638076782, val perplexity:6.059988266193678\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(os.getcwd()+'/log', comment='intent_slot')\n",
    "\n",
    "\n",
    "source_emb_dim = 64\n",
    "hidden_dim = 128\n",
    "n_layers = 1\n",
    "dropout = 0.5\n",
    "lr = 0.01\n",
    "gamma = 0.1\n",
    "weight_decay = 0.1\n",
    "n_epochs = 10\n",
    "clip = 5.0\n",
    "seq_len = 50\n",
    "model_path = os.path.join(os.getcwd(), \"model.h5\")\n",
    "\n",
    "\n",
    "model, optimizer, scheduler, loss_slot, loss_intent = build_model(SOURCE,\n",
    "                                                                  TARGET,\n",
    "                                                                  LABEL,\n",
    "                                                                  source_emb_dim,\n",
    "                                                                  hidden_dim,\n",
    "                                                                  n_layers,\n",
    "                                                                  dropout,\n",
    "                                                                  lr,\n",
    "                                                                  gamma,\n",
    "                                                                  weight_decay,\n",
    "                                                                  seq_len)\n",
    "\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "\n",
    "train_model(model,\n",
    "            train_iter,\n",
    "            val_iter,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            loss_slot, \n",
    "            loss_intent,\n",
    "            n_epochs,\n",
    "            clip,\n",
    "            model_path,\n",
    "            writer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
